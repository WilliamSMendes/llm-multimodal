import os
import cv2
import time
import json
import ffmpeg
import openai
import requests
import subprocess
from PIL import Image
from typing import List
from pathlib import Path
from openai import OpenAI
from huggingface_hub import snapshot_download
from faster_whisper import WhisperModel
from moviepy.editor import CompositeAudioClip
from moviepy.editor import concatenate_audioclips
from moviepy.editor import VideoFileClip, AudioFileClip
from .moondream.moondream import VisionEncoder, TextModel

# ANSI escape codes for colors, for styling the terminal output
PINK = '\033[95m'
CYAN = '\033[96m'
YELLOW = '\033[93m'
NEON_GREEN = '\033[92m'
RESET_COLOR = '\033[0m'

#Parameters
video_path = r"video/input/"
output_folder = r"video/output/"
mp3_file_path = r"video/mp3/"

# Initialize the OpenAI client with the API key
client = OpenAI(base_url="http://localhost:1234/v1", api_key="not-needed")

def mistral_model(user_input):
    """
    This function takes a user input and uses it to create a chat completion with a local model.
    The completion is streamed and processed chunk by chunk. Each chunk's content is added to a buffer.
    When a newline character is found in the buffer, the buffer is split into lines.
    Each line (except the last one) is printed in neon green and added to the full response.
    The last line is kept in the buffer for further processing.
    If there's any content left in the buffer after processing all chunks, it's printed and added to the full response.
    The full response is then returned.

    Parameters:
    user_input (str): The user's input to the chat model.

    Returns:
    str: The full response from the chat model.
    """
    streamed_completion = client.chat.completions.create(
        model="local-model",
        messages=[
            {"role": "system", "content": "You are a great writer."},
            {"role": "user", "content": user_input}
        ],
        stream=True  # Enable streaming
    )

    full_response = ""
    line_buffer = ""

    for chunk in streamed_completion:
        delta_content = chunk.choices[0].delta.content

        if delta_content is not None:
            line_buffer += delta_content

            if '\n' in line_buffer:
                lines = line_buffer.split('\n')
                for line in lines[:-1]:
                    print(NEON_GREEN + line + RESET_COLOR)
                    full_response += line + '\n'
                line_buffer = lines[-1]

    if line_buffer:
        print(NEON_GREEN + line_buffer + RESET_COLOR)
        full_response += line_buffer

    return full_response

# ANSI escape codes for colors, for styling the terminal output
# PINK = '\033[95m'
# CYAN = '\033[96m'
# YELLOW = '\033[93m'
# NEON_GREEN = '\033[92m'
# RESET_COLOR = '\033[0m'

def process_images(folder_path: str) -> tuple[TextModel, list]:
    """
    This function processes all images in a given folder. It uses a vision encoder and a text model
    to generate a description for each image. The description is generated by asking the text model
    to answer a question about the image.

    Parameters:
    folder_path (str): The path to the folder containing the images.

    Returns:
    TextModel: The text model used to generate the descriptions.
    list: A list of descriptions for all images in the folder.
    """
    model_path = snapshot_download("vikhyatk/moondream1")
    vision_encoder = VisionEncoder(model_path)
    text_model = TextModel(model_path)

    descriptions = []  # This will hold descriptions of all images

    # Iterate over each file in the folder
    for filename in os.listdir(folder_path):
        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):
            image_path = os.path.join(folder_path, filename)
            image = Image.open(image_path)
            # Optionally, display the image
            # image.show()
            image_embeds = vision_encoder(image)
            description = text_model.answer_question(image_embeds, "Identify the person in the image with thier full name:")
            descriptions.append(description)
    
    return text_model, descriptions

def convert_mp4_to_mp3(mp4_file_path, mp3_file_path):
    """
    This function converts an MP4 file to MP3 format using the moviepy library.

    The function works by first loading the MP4 file into a VideoFileClip object. This object represents
    a video clip and provides methods for manipulating the video. The audio of the video clip is then extracted
    into an AudioClip object using the audio attribute of the VideoFileClip object.

    The audio clip is then written to an MP3 file using the write_audiofile method of the AudioClip object.
    This method takes the path to the output file as an argument and writes the audio data to this file in MP3 format.

    After the audio has been written to the MP3 file, the VideoFileClip and AudioClip objects are closed to free up resources.

    If any error occurs during this process, the function catches the exception, prints an error message, and returns False.
    If the conversion is successful, the function returns True.

    Args:
    mp4_file_path (str): The path to the MP4 file to be converted. This should be a valid path to a file in MP4 format.
    mp3_file_path (str): The desired output path for the MP3 file. This should be a valid path where the output file will be written.

    Returns:
    bool: True if the conversion was successful, False otherwise. If False is returned, an error message will also be printed to the console.
    """
    try:
        # Load the MP4 file
        video_clip = VideoFileClip(mp4_file_path)

        # Extract audio from the video clip
        audio_clip = video_clip.audio

        # Write the audio to an MP3 file
        audio_clip.write_audiofile(mp3_file_path)

        # Close the clips
        video_clip.close()
        audio_clip.close()

        return True

    except Exception as e:
        print(f"An error occurred: {e}")
        return False
 
def transcribe_chunk(model, file_path):
    """
    This function transcribes a chunk of audio using a given model.

    The function uses the transcribe method of the model to transcribe the audio. The transcribe method
    returns a list of segments and some additional information. Each segment represents a portion of the audio
    and contains the transcribed text.

    The texts of all segments are then joined together with spaces in between to form the final transcription.
    This transcription is then returned.

    Parameters:
    model (Model): The model to use for transcription. This should be an object that has a transcribe method.
    file_path (str): The path to the audio file to transcribe. This should be a valid path to a file in a format that the model can handle.

    Returns:
    str: The transcription of the audio.
    """
    segments, info = model.transcribe(file_path, beam_size=7)
    transcription = ' '.join(segment.text for segment in segments)
    return transcription
 
def extract_frames(video_path, output_folder, frame_interval=120):
    """
    This function extracts frames from a video at specified intervals and saves them as JPEG images.

    The function first checks if the output folder exists and creates it if it doesn't. It then opens the video file
    using OpenCV's VideoCapture class. If the video file cannot be opened, an error message is printed and an empty list is returned.

    The function then reads frames from the video one by one. If a frame cannot be read (which indicates the end of the video),
    the function breaks out of the loop.

    For each frame, the function checks if the frame count is a multiple of the frame interval. If it is, the frame is written
    to a JPEG file in the output folder. The path to the JPEG file is then added to a list of extracted frame paths.

    After all frames have been processed, the VideoCapture object is released to free up resources. A message indicating the
    completion of the frame extraction and the number of frames extracted is then printed.

    The function finally returns the list of paths to the extracted frames.

    Parameters:
    video_path (str): The path to the video file to extract frames from. This should be a valid path to a file in a format that OpenCV can handle.
    output_folder (str): The path to the folder where the extracted frames will be saved. If this folder does not exist, it will be created.
    frame_interval (int, optional): The interval between frames to extract. Defaults to 120.

    Returns:
    list: A list of paths to the extracted frames.
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("Error: Could not open video.")
        return []

    frame_count = 0
    extracted_frame_paths = []
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        if frame_count % frame_interval == 0:
            frame_filename = f'{output_folder}/frame_{frame_count}.jpg'
            cv2.imwrite(frame_filename, frame)
            extracted_frame_paths.append(frame_filename)
        
        frame_count += 1

    cap.release()
    print(f"Frame extraction complete. {len(extracted_frame_paths)} frames extracted.")
    return extracted_frame_paths

# Main execution
convert_mp4_to_mp3(video_path, mp3_file_path)
    
def main():
    """
    This is the main function that orchestrates the process of transcribing audio, extracting frames from a video, 
    processing images to obtain descriptions, and generating a summary.

    The function first sets up a WhisperModel with specified settings for transcribing audio. It then transcribes 
    the audio from an MP3 file and prints the transcription.

    Next, the function extracts frames from a video file and saves them in an output folder.

    The function then processes the images in the output folder using a TextModel to obtain descriptions for each image. 
    These descriptions are joined into a single string.

    The function then constructs a string that contains the video and audio descriptions and asks a Mistral model to 
    generate a detailed summary based on these descriptions.

    Finally, the function prints the generated summary.

    Parameters:
    None

    Returns:
    None
    """
    # Choose your model settings
    model_size = "medium.en"
    model = WhisperModel(model_size, device="cuda", compute_type="float16")
    transcription = transcribe_chunk(model, mp3_file_path)
    print(transcription)
    
    frame_paths = extract_frames(video_path, output_folder)
    
    # Process images in the folder and obtain descriptions
    text_model, descriptions = process_images(output_folder)
    
    # Join all descriptions into a single string variable
    all_descriptions = ' '.join(descriptions)
    
    explain = f"VIDEO DESCRIPTION = {all_descriptions} \n #AUDIO DESCRIPTION = {transcription} \n From the VIDEO DESCRIPTION and AUDIO DESCRIPTION above, write detailed summary:"
    desc = mistral_model(explain)
    
    # Print the combined descriptions
    print(f"{CYAN}Video Summary:{RESET_COLOR} {NEON_GREEN}{desc}{RESET_COLOR}\n")
    
if __name__ == "__main__":
    main()
